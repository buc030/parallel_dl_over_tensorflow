{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.ops import variables\n",
    "\n",
    "class HVar:\n",
    "    #this contains all alphas in the graph\n",
    "    all_hvars = []\n",
    "    \n",
    "    def __init__(self, var, hSize = 2):\n",
    "        self.name = var.name.split(\":\")[0].split(\"/\")[-1]\n",
    "        \n",
    "        with tf.name_scope(self.name + '_history'):\n",
    "            self.var = var\n",
    "            self.replicas = [] #this taks 2X memory\n",
    "            self.aplha = []\n",
    "            self.last_snapshot = tf.Variable(var.initialized_value(), name='snapshot') #this makes it 3X + hSize\n",
    "            self.next_idx = 0\n",
    "            self.op_cache = {}\n",
    "\n",
    "            #counter = tf.Variable(0, dtype=tf.int32, name='sesop_counter')\n",
    "            for i in range(hSize):\n",
    "                #self.replicas[tf.placeholder(shape=var.get_shape(), dtype=tf.float32)] =\\\n",
    "                #    np.zeros(var.get_shape())\n",
    "                self.replicas.append(tf.Variable(np.zeros(var.get_shape()), dtype=var.dtype.base_dtype, name='replica'))\n",
    "                self.aplha.append(tf.Variable(np.zeros(1), dtype=var.dtype.base_dtype, name='alpha'))\n",
    "\n",
    "\n",
    "            for i in range(hSize):\n",
    "                self.push_history_op() #make sure all ops are created\n",
    "\n",
    "            HVar.all_hvars.append(self)\n",
    "            assert(self.next_idx == 0)\n",
    "\n",
    "        \n",
    "    def out(self):\n",
    "        with tf.name_scope(self.name + '_out'):\n",
    "            #return an affine combination of the history vectors\n",
    "            #and a dictonary to add to feed_dict.\n",
    "            self.o = self.var\n",
    "            for r, a in zip(self.replicas, self.aplha):\n",
    "                self.o += r*a\n",
    "\n",
    "            return self.o\n",
    "        \n",
    "    #returns an op that updates history and snapshot (executed after optimization on alpha)\n",
    "    def push_history_op(self):\n",
    "        if self.next_idx not in self.op_cache:\n",
    "            print 'HVar Cache Miss, creating the op for var ' + str(self.var.name) + ', idx = ' + str(self.next_idx)\n",
    "            \n",
    "            with tf.name_scope(self.name + '_update'):\n",
    "                update_history_op = tf.assign(self.replicas[self.next_idx], self.out() - self.last_snapshot)\n",
    "                with tf.control_dependencies([update_history_op]):\n",
    "                    update_snapshot_op = tf.assign(self.last_snapshot, self.out())\n",
    "                    update_var_op = tf.assign(self.var, self.out())\n",
    "\n",
    "            self.op_cache[self.next_idx] = tf.group(update_history_op, update_var_op, update_snapshot_op)\n",
    "            \n",
    "        old_idx = self.next_idx\n",
    "        self.next_idx = (self.next_idx + 1)%len(self.replicas)\n",
    "\n",
    "        return self.op_cache[old_idx]\n",
    "    \n",
    "    @classmethod\n",
    "    def all_trainable_alphas(self):\n",
    "        alphas = []\n",
    "        for hvar in HVar.all_hvars:\n",
    "            alphas.extend(hvar.aplha)\n",
    "        return alphas\n",
    "    \n",
    "    @classmethod\n",
    "    def all_history_update_ops(self):\n",
    "        group_op = tf.no_op()\n",
    "        for hvar in HVar.all_hvars:\n",
    "            group_op = tf.group(group_op, hvar.push_history_op())\n",
    "            \n",
    "        return group_op\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class SeboostOptimizer:\n",
    "    #batched_input, batched_labels are tensors that prodece batches\n",
    "    #is_training is a tensor that will be true while training and false while testing\n",
    "    #we run CG once in sesop_freq iterations \n",
    "    def __init__(self, loss, batched_input, batched_labels, sesop_freq):\n",
    "        self.train_step = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss, name='minimizer')\n",
    "        self.loss = loss\n",
    "        self.train_loss = []\n",
    "        self.loss_before_sesop = []\n",
    "        self.loss_after_sesop = []\n",
    "        self.history_update_ops = HVar.all_history_update_ops()\n",
    "        self.sesop_freq = sesop_freq\n",
    "        self.sesop_iteration_ran = 0\n",
    "        self.avg_gain_from_cg = 0.0\n",
    "        \n",
    "        self.cg = tf.contrib.opt.ScipyOptimizerInterface(loss, var_list=HVar.all_trainable_alphas(),\\\n",
    "            method='CG', options={'maxiter':5})\n",
    "        \n",
    "        self.batched_input, self.batched_labels = batched_input, batched_labels\n",
    "        \n",
    "    #_feed_dict is the feed_dict needed to run regular sgd iteration\n",
    "    #sesop_feed_dict should contain feeds for the batch sesop will use!\n",
    "    #return a list of train_loss. The last elment in the list contain the loss after sesop.\n",
    "    def run_sesop_iteration(self, sess, _feed_dict, sesop_feed_dict):\n",
    "        #run sesop_freq SGD iterations:\n",
    "        train_loss = []\n",
    "\n",
    "            \n",
    "        for i in range(self.sesop_freq):\n",
    "            _, loss = sess.run([self.train_step, self.loss], feed_dict=_feed_dict)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            s = sess.run(self.iter_summaries, feed_dict=_feed_dict)\n",
    "            self.writer.add_summary(s, self.sesop_iteration_ran*(self.sesop_freq + 1) + i)\n",
    "        \n",
    "        self.train_loss.extend(train_loss)\n",
    "        self.loss_before_sesop.append(train_loss[-1])\n",
    "        #run 1 CG iteration\n",
    "        self.cg.minimize(sess, feed_dict=sesop_feed_dict)\n",
    "        \n",
    "        self.loss_after_sesop.append(sess.run(self.loss, feed_dict=sesop_feed_dict))\n",
    "        \n",
    "        self.avg_gain_from_cg += self.loss_before_sesop[-1] - self.loss_after_sesop[-1] \n",
    "        print 'Gain from CG: ' + str(self.avg_gain_from_cg/self.sesop_iteration_ran)\n",
    "        train_loss.append(self.loss_after_sesop[-1])\n",
    "        #Now when alphas are optimized, run the update history ops:\n",
    "        sess.run(self.history_update_ops)\n",
    "        \n",
    "        s = sess.run(self.iter_summaries, feed_dict=_feed_dict)\n",
    "        self.writer.add_summary(s, self.sesop_iteration_ran*(self.sesop_freq + 1) + self.sesop_freq)\n",
    "            \n",
    "        self.sesop_iteration_ran += 1\n",
    "        return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dim = 10\n",
    "#with tf.name_scope('scope'):\n",
    "#    x = tf.Variable(tf.random_normal([dim, 1]), tf.float32, name='x')\n",
    "\n",
    "print x.name.split(\":\")[0].split(\"/\")[-1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nself._packed_var\\n\\nwith tf.Session() as sess:\\n    \\n    sess.run(tf.global_variables_initializer())\\n    sess.run(tf.local_variables_initializer())\\n    \\n    for i in range(20):\\n        iter_loss = sess.run(loss)\\n        print 'loss = '  + str(iter_loss)\\n        sess.run(train_step)\\n        \\n        #op.minimize(sess)\\n        \\nprint '----------------'\\n\\nwith tf.Session() as sess:\\n    \\n    sess.run(tf.global_variables_initializer())\\n    sess.run(tf.local_variables_initializer())\\n    \\n    for i in range(20):\\n        iter_loss = sess.run(loss)\\n        print 'loss = '  + str(iter_loss)\\n        op.minimize(sess)\\n        \\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 10\n",
    "\n",
    "#quad function\n",
    "\n",
    "x = tf.Variable(tf.random_normal([dim, 1]), tf.float32, name='x')\n",
    "HVar(x)\n",
    "\n",
    "A = tf.random_normal([dim, dim], name='A')\n",
    "A = tf.matmul(A, tf.transpose(A))\n",
    "b = tf.random_normal([dim, 1], name='b')\n",
    "c = tf.random_normal([1, 1], name='c')\n",
    "\n",
    "model_out = tf.matmul(tf.transpose(x), tf.matmul(A, x)) + tf.matmul(tf.transpose(b), x) + c\n",
    "loss = model_out\n",
    "\n",
    "#loss = model_out*model_out\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss, name='minimizer')\n",
    "op = tf.contrib.opt.ScipyOptimizerInterface(loss, method='CG', options={'maxiter': 300, 'gtol': 0.00001})\n",
    "\n",
    "print op._packed_var.shape\n",
    "\"\"\"\n",
    "self._packed_var\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    for i in range(20):\n",
    "        iter_loss = sess.run(loss)\n",
    "        print 'loss = '  + str(iter_loss)\n",
    "        sess.run(train_step)\n",
    "        \n",
    "        #op.minimize(sess)\n",
    "        \n",
    "print '----------------'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    for i in range(20):\n",
    "        iter_loss = sess.run(loss)\n",
    "        print 'loss = '  + str(iter_loss)\n",
    "        op.minimize(sess)\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 != 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
