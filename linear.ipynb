{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard: no process found\n",
      "<class 'tensorflow.contrib.opt.python.training.external_optimizer.ScipyOptimizerInterface'>\n"
     ]
    }
   ],
   "source": [
    "!killall tensorboard\n",
    "!rm /tmp/generated_data/1 -rf\n",
    "\n",
    "import tensorflow as tf\n",
    "#contrib/opt/python/training/external_optimizer.py\n",
    "print tf.contrib.opt.ScipyOptimizerInterface\n",
    "\n",
    "\n",
    "class SummaryManager:\n",
    "    def __init__(self):\n",
    "        self.iter_summaries = []\n",
    "\n",
    "    def add_iter_summary(self, s):\n",
    "        self.iter_summaries.append(s)\n",
    "        \n",
    "    def merge_iters(self):\n",
    "        return tf.summary.merge(self.iter_summaries)\n",
    "    \n",
    "    \n",
    "\n",
    "class HVar:\n",
    "    #this contains all alphas in the graph\n",
    "    all_hvars = []\n",
    "    \n",
    "    def __init__(self, var, hSize = 2):\n",
    "        self.name = var.name.split(\":\")[0].split(\"/\")[-1]\n",
    "        \n",
    "        with tf.name_scope(self.name + '_history'):\n",
    "            self.var = var\n",
    "            self.replicas = [] #this taks 2X memory\n",
    "            self.aplha = []\n",
    "            self.last_snapshot = tf.Variable(var.initialized_value(), name='snapshot') #this makes it 3X + hSize\n",
    "            self.next_idx = 0\n",
    "            self.op_cache = {}\n",
    "\n",
    "            #counter = tf.Variable(0, dtype=tf.int32, name='sesop_counter')\n",
    "            for i in range(hSize):\n",
    "                #self.replicas[tf.placeholder(shape=var.get_shape(), dtype=tf.float32)] =\\\n",
    "                #    np.zeros(var.get_shape())\n",
    "                self.replicas.append(tf.Variable(np.zeros(var.get_shape()), dtype=var.dtype.base_dtype, name='replica'))\n",
    "                self.aplha.append(tf.Variable(np.zeros(1), dtype=var.dtype.base_dtype, name='alpha'))\n",
    "\n",
    "\n",
    "            for i in range(hSize):\n",
    "                self.push_history_op() #make sure all ops are created\n",
    "\n",
    "            HVar.all_hvars.append(self)\n",
    "            assert(self.next_idx == 0)\n",
    "\n",
    "        \n",
    "    def out(self):\n",
    "        with tf.name_scope(self.name + '_out'):\n",
    "            #return an affine combination of the history vectors\n",
    "            #and a dictonary to add to feed_dict.\n",
    "            self.o = self.var\n",
    "            for r, a in zip(self.replicas, self.aplha):\n",
    "                self.o += r*a\n",
    "\n",
    "            return self.o\n",
    "        \n",
    "    #returns an op that updates history and snapshot (executed after optimization on alpha)\n",
    "    def push_history_op(self):\n",
    "        if self.next_idx not in self.op_cache:\n",
    "            print 'HVar Cache Miss, creating the op for var ' + str(self.var.name) + ', idx = ' + str(self.next_idx)\n",
    "            \n",
    "            with tf.name_scope(self.name + '_update'):\n",
    "                update_history_op = tf.assign(self.replicas[self.next_idx], self.out() - self.last_snapshot)\n",
    "                with tf.control_dependencies([update_history_op]):\n",
    "                    update_snapshot_op = tf.assign(self.last_snapshot, self.out())\n",
    "                    update_var_op = tf.assign(self.var, self.out())\n",
    "\n",
    "            self.op_cache[self.next_idx] = tf.group(update_history_op, update_var_op, update_snapshot_op)\n",
    "            \n",
    "        old_idx = self.next_idx\n",
    "        self.next_idx = (self.next_idx + 1)%len(self.replicas)\n",
    "\n",
    "        return self.op_cache[old_idx]\n",
    "    \n",
    "    @classmethod\n",
    "    def all_trainable_alphas(self):\n",
    "        alphas = []\n",
    "        for hvar in HVar.all_hvars:\n",
    "            alphas.extend(hvar.aplha)\n",
    "        return alphas\n",
    "    \n",
    "    @classmethod\n",
    "    def all_history_update_ops(self):\n",
    "        group_op = tf.no_op()\n",
    "        for hvar in HVar.all_hvars:\n",
    "            group_op = tf.group(group_op, hvar.push_history_op())\n",
    "            \n",
    "        return group_op\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class SeboostOptimizer:\n",
    "    #batched_input, batched_labels are tensors that prodece batches\n",
    "    #is_training is a tensor that will be true while training and false while testing\n",
    "    #we run CG once in sesop_freq iterations \n",
    "    def __init__(self, loss, batched_input, batched_labels, sesop_freq):\n",
    "        self.train_step = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss, name='minimizer')\n",
    "        self.loss = loss\n",
    "        self.train_loss = []\n",
    "        self.loss_before_sesop = []\n",
    "        self.loss_after_sesop = []\n",
    "        self.history_update_ops = HVar.all_history_update_ops()\n",
    "        self.sesop_freq = sesop_freq\n",
    "        self.sesop_iteration_ran = 0\n",
    "        self.avg_gain_from_cg = 0.0\n",
    "        \n",
    "        self.cg = tf.contrib.opt.ScipyOptimizerInterface(loss, var_list=HVar.all_trainable_alphas(),\\\n",
    "            method='CG', options={'maxiter':5})\n",
    "        \n",
    "        self.batched_input, self.batched_labels = batched_input, batched_labels\n",
    "        \n",
    "    #_feed_dict is the feed_dict needed to run regular sgd iteration\n",
    "    #sesop_feed_dict should contain feeds for the batch sesop will use!\n",
    "    #return a list of train_loss. The last elment in the list contain the loss after sesop.\n",
    "    def run_sesop_iteration(self, sess, _feed_dict, sesop_feed_dict):\n",
    "        #run sesop_freq SGD iterations:\n",
    "        train_loss = []\n",
    "\n",
    "            \n",
    "        for i in range(self.sesop_freq):\n",
    "            _, loss = sess.run([self.train_step, self.loss], feed_dict=_feed_dict)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            s = sess.run(self.iter_summaries, feed_dict=_feed_dict)\n",
    "            self.writer.add_summary(s, self.sesop_iteration_ran*(self.sesop_freq + 1) + i)\n",
    "        \n",
    "        self.train_loss.extend(train_loss)\n",
    "        self.loss_before_sesop.append(train_loss[-1])\n",
    "        #run 1 CG iteration\n",
    "        self.cg.minimize(sess, feed_dict=sesop_feed_dict)\n",
    "        \n",
    "        self.loss_after_sesop.append(sess.run(self.loss, feed_dict=sesop_feed_dict))\n",
    "        \n",
    "        self.avg_gain_from_cg += self.loss_before_sesop[-1] - self.loss_after_sesop[-1] \n",
    "        print 'Gain from CG: ' + str(self.avg_gain_from_cg/self.sesop_iteration_ran)\n",
    "        train_loss.append(self.loss_after_sesop[-1])\n",
    "        #Now when alphas are optimized, run the update history ops:\n",
    "        sess.run(self.history_update_ops)\n",
    "        \n",
    "        s = sess.run(self.iter_summaries, feed_dict=_feed_dict)\n",
    "        self.writer.add_summary(s, self.sesop_iteration_ran*(self.sesop_freq + 1) + self.sesop_freq)\n",
    "            \n",
    "        self.sesop_iteration_ran += 1\n",
    "        return train_loss\n",
    "        \n",
    "        \n",
    "summaryMgr = SummaryManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def fc_layer(input, n_in, n_out, log):\n",
    "    with tf.name_scope('FC'):\n",
    "        W = HVar(tf.Variable(tf.random_normal([n_in, n_out]), name='W'))\n",
    "        b = HVar(tf.Variable(tf.zeros([n_out]), name='b'))\n",
    "\n",
    "        \n",
    "        a = tf.matmul(input, W.out()) + b.out()\n",
    "        \n",
    "        out = tf.nn.tanh(a)\n",
    "        \n",
    "        if log:\n",
    "            summaryMgr.add_iter_summary(tf.summary.histogram('activations_before_tanh', a))\n",
    "            summaryMgr.add_iter_summary(tf.summary.histogram('activations_after_tanh', out))\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define the model:\n",
    "\n",
    "\n",
    "def build_model(x, y, dim, log=False):\n",
    "    layers = [fc_layer(x, dim, dim, log)]\n",
    "    for i in range(1):\n",
    "        layers.append(fc_layer(layers[-1], dim, dim, log))\n",
    "    layers.append(fc_layer(layers[-1], dim, 1, log))\n",
    "\n",
    "    model_out = layers[-1]\n",
    "\n",
    "\n",
    "    \n",
    "    #when log is true we build a model for training!\n",
    "    if log:\n",
    "\n",
    "        loss_per_sample = tf.squared_difference(model_out, y, name='loss_per_sample')\n",
    "        loss = tf.reduce_mean(loss_per_sample, name='loss')\n",
    "        summaryMgr.add_iter_summary(tf.summary.scalar('loss', loss))\n",
    "\n",
    "        return model_out, loss\n",
    "    #tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    return model_out #, loss, train_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_random_data(dim, n=5000):\n",
    "    cov = np.random.rand(dim, dim)\n",
    "    cov = np.dot(cov, cov.transpose())\n",
    "\n",
    "    training_data = np.random.multivariate_normal(np.zeros(dim), cov, n)\n",
    "    testing_data = np.random.multivariate_normal(np.zeros(dim), cov, n)\n",
    "    \n",
    "    with tf.name_scope('generating_data'):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, dim], name='x')\n",
    "        model_out = build_model(x, None, dim, False)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            training_labels = sess.run(model_out, feed_dict={x: training_data})\n",
    "            testing_labels = sess.run(model_out, feed_dict={x: testing_data})\n",
    "\n",
    "        return training_data, testing_data, training_labels, testing_labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HVar Cache Miss, creating the op for var generating_data/FC/W:0, idx = 0\n",
      "HVar Cache Miss, creating the op for var generating_data/FC/W:0, idx = 1\n",
      "HVar Cache Miss, creating the op for var generating_data/FC/b:0, idx = 0\n",
      "HVar Cache Miss, creating the op for var generating_data/FC/b:0, idx = 1\n",
      "HVar Cache Miss, creating the op for var generating_data/FC_1/W:0, idx = 0\n",
      "HVar Cache Miss, creating the op for var generating_data/FC_1/W:0, idx = 1\n",
      "HVar Cache Miss, creating the op for var generating_data/FC_1/b:0, idx = 0\n",
      "HVar Cache Miss, creating the op for var generating_data/FC_1/b:0, idx = 1\n",
      "HVar Cache Miss, creating the op for var generating_data/FC_2/W:0, idx = 0\n",
      "HVar Cache Miss, creating the op for var generating_data/FC_2/W:0, idx = 1\n",
      "HVar Cache Miss, creating the op for var generating_data/FC_2/b:0, idx = 0\n",
      "HVar Cache Miss, creating the op for var generating_data/FC_2/b:0, idx = 1\n",
      "(5000, 10)\n",
      "(5000, 10)\n",
      "HVar Cache Miss, creating the op for var FC/W:0, idx = 0\n",
      "HVar Cache Miss, creating the op for var FC/W:0, idx = 1\n",
      "HVar Cache Miss, creating the op for var FC/b:0, idx = 0\n",
      "HVar Cache Miss, creating the op for var FC/b:0, idx = 1\n",
      "HVar Cache Miss, creating the op for var FC_1/W:0, idx = 0\n",
      "HVar Cache Miss, creating the op for var FC_1/W:0, idx = 1\n",
      "HVar Cache Miss, creating the op for var FC_1/b:0, idx = 0\n",
      "HVar Cache Miss, creating the op for var FC_1/b:0, idx = 1\n",
      "HVar Cache Miss, creating the op for var FC_2/W:0, idx = 0\n",
      "HVar Cache Miss, creating the op for var FC_2/W:0, idx = 1\n",
      "HVar Cache Miss, creating the op for var FC_2/b:0, idx = 0\n",
      "HVar Cache Miss, creating the op for var FC_2/b:0, idx = 1\n",
      "epoch #0\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -inf\n",
      "epoch #1\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.175102449954\n",
      "epoch #2\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0555935874581\n",
      "epoch #3\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0645877470573\n",
      "epoch #4\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0411982573569\n",
      "epoch #5\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0217428296804\n",
      "epoch #6\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.027239613235\n",
      "epoch #7\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0608820127589\n",
      "epoch #8\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0827860515565\n",
      "epoch #9\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0855820791589\n",
      "epoch #10\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0583339378238\n",
      "epoch #11\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0649047073993\n",
      "epoch #12\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0656973856191\n",
      "epoch #13\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0686382364768\n",
      "epoch #14\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0697921163269\n",
      "epoch #15\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0791124572357\n",
      "epoch #16\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0540443202481\n",
      "epoch #17\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0650516494232\n",
      "epoch #18\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0589699703786\n",
      "epoch #19\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0685902731983\n",
      "epoch #20\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0921439342201\n",
      "epoch #21\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0932723901102\n",
      "epoch #22\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0945968282494\n",
      "epoch #23\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0849249913641\n",
      "epoch #24\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0753252425541\n",
      "epoch #25\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0629044550657\n",
      "epoch #26\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0506410042827\n",
      "epoch #27\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0456040981743\n",
      "epoch #28\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0457548280912\n",
      "epoch #29\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0289831361894\n",
      "epoch #30\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0377481912573\n",
      "epoch #31\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0305534413745\n",
      "epoch #32\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.02946680272\n",
      "epoch #33\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0337157958385\n",
      "epoch #34\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0335313647109\n",
      "epoch #35\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0356682424034\n",
      "epoch #36\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0321401014096\n",
      "epoch #37\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0301672788085\n",
      "epoch #38\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.034548203804\n",
      "epoch #39\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0308795567506\n",
      "epoch #40\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0336062621325\n",
      "epoch #41\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.03083630415\n",
      "epoch #42\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: -0.0269395194593\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Run call was cancelled\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Infinity in summary histogram for: FC/activations_before_tanh\n\t [[Node: FC/activations_before_tanh = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](FC/activations_before_tanh/tag, FC/add/_105)]]\n\nCaused by op u'FC/activations_before_tanh', defined at:\n  File \"/home/shai/anaconda/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/shai/anaconda/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/tornado/ioloop.py\", line 866, in start\n    handler_func(fd_obj, events)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2705, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2809, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2869, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-3f6564a26bf4>\", line 62, in <module>\n    model_out, loss = build_model(batched_input, batched_labels, dim, True)\n  File \"<ipython-input-3-74caaac83eac>\", line 5, in build_model\n    layers = [fc_layer(x, dim, dim, log)]\n  File \"<ipython-input-2-cdd5782459ea>\", line 13, in fc_layer\n    summaryMgr.add_iter_summary(tf.summary.histogram('activations_before_tanh', a))\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/tensorflow/python/summary/summary.py\", line 203, in histogram\n    tag=scope.rstrip('/'), values=values, name=scope)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 139, in _histogram_summary\n    name=name)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Infinity in summary histogram for: FC/activations_before_tanh\n\t [[Node: FC/activations_before_tanh = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](FC/activations_before_tanh/tag, FC/add/_105)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3f6564a26bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;31m#this runs 100 regular iterations + 1 CG iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m#optimizer.run_sesop_iteration(self, sess, sesop_feed_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0msesop_iter_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_sesop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_feed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0msesop_feed_dict\u001b[0m\u001b[0;34m=\u001b[0m                \u001b[0;34m{\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatched_input_actual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatched_labels_actual\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-8f868dfec3ab>\u001b[0m in \u001b[0;36mrun_sesop_iteration\u001b[0;34m(self, sess, _feed_dict, sesop_feed_dict)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory_update_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_summaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msesop_iteration_ran\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msesop_freq\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msesop_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shai/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shai/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shai/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/shai/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Infinity in summary histogram for: FC/activations_before_tanh\n\t [[Node: FC/activations_before_tanh = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](FC/activations_before_tanh/tag, FC/add/_105)]]\n\nCaused by op u'FC/activations_before_tanh', defined at:\n  File \"/home/shai/anaconda/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/shai/anaconda/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/tornado/ioloop.py\", line 866, in start\n    handler_func(fd_obj, events)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2705, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2809, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2869, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-3f6564a26bf4>\", line 62, in <module>\n    model_out, loss = build_model(batched_input, batched_labels, dim, True)\n  File \"<ipython-input-3-74caaac83eac>\", line 5, in build_model\n    layers = [fc_layer(x, dim, dim, log)]\n  File \"<ipython-input-2-cdd5782459ea>\", line 13, in fc_layer\n    summaryMgr.add_iter_summary(tf.summary.histogram('activations_before_tanh', a))\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/tensorflow/python/summary/summary.py\", line 203, in histogram\n    tag=scope.rstrip('/'), values=values, name=scope)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 139, in _histogram_summary\n    name=name)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/shai/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Infinity in summary histogram for: FC/activations_before_tanh\n\t [[Node: FC/activations_before_tanh = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](FC/activations_before_tanh/tag, FC/add/_105)]]\n"
     ]
    }
   ],
   "source": [
    "dim = 10\n",
    "#None for the batches\n",
    "#x = tf.placeholder(tf.float32, shape=[None, dim], name='x')\n",
    "#y = tf.placeholder(tf.float32, shape=[None, 1], name='y')\n",
    "\n",
    "\n",
    "training_data, testing_data, training_labels, testing_labels = generate_random_data(dim, 5000)\n",
    "\n",
    "#print 'training_data = ' + str(training_data)\n",
    "#print 'testing_data = ' + str(testing_data)\n",
    "#print 'training_labels = ' + str(training_labels)\n",
    "#print 'testing_labels = ' + str(testing_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#numpy.random.multivariate_normal() #mean, cov[, size])\n",
    "\n",
    "#generate random covariance matrix, must be semi-positive definite, so we multiply it by its transpose\n",
    "\n",
    "        \n",
    "print training_data.shape\n",
    "print testing_data.shape\n",
    "\n",
    "#batch_size\n",
    "bs = 100\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('data'):\n",
    "        is_training = tf.placeholder(tf.bool,name='is_training') #must be feed with dict_feed.\n",
    "        \n",
    "        def create_training_dataset():\n",
    "            inputs = tf.cast(tf.constant(training_data, name='train_dataset_x'), tf.float32)\n",
    "            labels = tf.cast(tf.constant(training_labels, name='train_dataset_y'), tf.float32)\n",
    "            input, label = tf.train.slice_input_producer([inputs, labels], name='train_slicer')\n",
    "            batched_input, batched_labels = \\\n",
    "                tf.train.batch([input, label], batch_size=bs, name='train_batcher')\n",
    "            return batched_input, batched_labels\n",
    "        \n",
    "        \n",
    "        \n",
    "        def create_testing_dataset():\n",
    "            inputs = tf.cast(tf.constant(testing_data, name='test_dataset_x'), tf.float32)\n",
    "            labels = tf.cast(tf.constant(testing_labels, name='test_dataset_y'), tf.float32)\n",
    "            \n",
    "            input, label = tf.train.slice_input_producer([inputs, labels], name='test_slicer')\n",
    "            batched_input, batched_labels = \\\n",
    "                tf.train.batch([input, label], batch_size=bs, name='test_batcher')\n",
    "            return batched_input, batched_labels\n",
    "        \n",
    "        \n",
    "        #It is very important to call create_training_dataset and create_testing_dataset \n",
    "        #create all queues (for train and test)\n",
    "        train_batched_input, train_batched_labels = create_training_dataset()\n",
    "        test_batched_input, test_batched_labels = create_testing_dataset()\n",
    "        \n",
    "        \n",
    "        batched_input, batched_labels = tf.cond(is_training, lambda: [train_batched_input, train_batched_labels],\\\n",
    "            lambda: [test_batched_input, test_batched_labels])\n",
    "        \n",
    "    \n",
    "    \n",
    "    model_out, loss = build_model(batched_input, batched_labels, dim, True)\n",
    "    \n",
    "    #class SeboostOptimizer:\n",
    "    #batched_input, batched_labels are tensors that prodece batches\n",
    "    #is_training is a tensor that will be true while training and false while testing\n",
    "    #def __init__(self, loss, batched_input, batched_labels):\n",
    "    #run sesop once an epoch\n",
    "    sesop_freq = 5000/bs\n",
    "    optimizer = SeboostOptimizer(loss, batched_input, batched_labels, sesop_freq)\n",
    "    \n",
    "    #method=’CG’\n",
    "\n",
    "    \n",
    "    #hold acc loss\n",
    "    with tf.name_scope('loss_accamulator'):\n",
    "        acc_loss = tf.Variable(0, name='acc_loss', dtype=tf.float32)\n",
    "        train_loss_summary = tf.summary.scalar('train_loss', acc_loss)\n",
    "        test_loss_summary = tf.summary.scalar('test_loss', acc_loss)\n",
    "    \n",
    "    iter_summaries = summaryMgr.merge_iters()\n",
    "    optimizer.iter_summaries = iter_summaries\n",
    "        \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    #merged_summery = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter('/tmp/generated_data/1')\n",
    "    writer.add_graph(sess.graph)\n",
    "    optimizer.writer = writer\n",
    "    \n",
    "    #we must start queue_runners\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    \n",
    "    #run 200 epochs:\n",
    "    for epoch in range(200):\n",
    "        #run 20 steps (full batch optimization to start with)\n",
    "        print 'epoch #' + str(epoch)\n",
    "            \n",
    "        print 'Computing train error'\n",
    "        #train error:\n",
    "        total_loss = 0\n",
    "        for i in range(0):\n",
    "            iter_loss = sess.run(loss, feed_dict={is_training: True})\n",
    "            total_loss += iter_loss\n",
    "            \n",
    "        #put the accamulated loss into acc_loss node\n",
    "        writer.add_summary(sess.run(train_loss_summary, feed_dict={acc_loss: total_loss/(5000/bs)}), epoch)\n",
    "        \n",
    "        print 'Computing test error'\n",
    "        #test error:\n",
    "        total_loss = 0\n",
    "        for i in range(0):\n",
    "            iter_loss = sess.run(loss, feed_dict={is_training: False})\n",
    "            total_loss += iter_loss\n",
    "            \n",
    "        #put the accamulated loss into acc_loss node\n",
    "        writer.add_summary(sess.run(test_loss_summary, feed_dict={acc_loss: total_loss/(5000/bs)}), epoch)\n",
    "        \n",
    "        print 'Training'\n",
    "        total_loss = 0\n",
    "        iters_per_epoch = 5000/bs\n",
    "        #train epoch (This actually extract 2 epochs out of the batcher, because of the summary).\n",
    "        #so in total, a full epoch loop extract 3 epochs, thus after 8 epochs we will extract 24 epochs,\n",
    "        #thus since we limited number of \n",
    "        #print 'iters_per_epoch = ' + str(iters_per_epoch) #iters_per_epoch = 50\n",
    "        assert(sesop_freq <= iters_per_epoch)\n",
    "        for i in range(iters_per_epoch/sesop_freq):\n",
    "            #_, iter_loss = sess.run([train_step, loss], feed_dict={is_training: True})\n",
    "            #take a batch:\n",
    "            batched_input_actual, batched_labels_actual = \\\n",
    "                sess.run([batched_input, batched_labels], feed_dict={is_training: True})\n",
    "                \n",
    "            #this runs 100 regular iterations + 1 CG iteration.\n",
    "            #optimizer.run_sesop_iteration(self, sess, sesop_feed_dict)\n",
    "            sesop_iter_loss = optimizer.run_sesop_iteration(sess=sess, _feed_dict={is_training: True} ,sesop_feed_dict=\\\n",
    "                {is_training: True, batched_input: batched_input_actual, batched_labels: batched_labels_actual})\n",
    "            \n",
    "            \n",
    "        writer.flush()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "\n",
    "    #res = sess.run(model_out, feed_dict={x: training_data})\n",
    "    \n",
    "    #print res\n",
    "    \n",
    "    \n",
    "    #tf.summary.scalar('model_out', model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bs = 50\n",
    "training_data[0:bs].shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
