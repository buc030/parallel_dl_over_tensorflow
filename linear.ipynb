{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard: no process found\n",
      "<class 'tensorflow.contrib.opt.python.training.external_optimizer.ScipyOptimizerInterface'>\n"
     ]
    }
   ],
   "source": [
    "!killall tensorboard\n",
    "!rm /tmp/generated_data/* -rf\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import argparse\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "#contrib/opt/python/training/external_optimizer.py\n",
    "print tf.contrib.opt.ScipyOptimizerInterface\n",
    "\n",
    "class SummaryManager:\n",
    "    def __init__(self):\n",
    "        self.iter_summaries = {}\n",
    "        self.models = []\n",
    "        \n",
    "    def add_iter_summary(self, s):\n",
    "        self.iter_summaries[self.models[-1]].append(s)\n",
    "        \n",
    "    def merge_iters(self):\n",
    "        return tf.summary.merge(self.iter_summaries[self.models[-1]])\n",
    "\n",
    "    #from the second a model is poped summaries are always added into this model.\n",
    "    def push_model(self, model):\n",
    "        self.models.append(model)\n",
    "        self.iter_summaries[model] = []\n",
    "        \n",
    "    def pop_model(self):\n",
    "        self.models.pop()\n",
    "        \n",
    "\n",
    "class HVar:\n",
    "    #this contains all alphas in the graph\n",
    "    all_hvars = []\n",
    "    \n",
    "    def __init__(self, var, hSize = 2):\n",
    "        self.name = var.name.split(\":\")[0].split(\"/\")[-1]\n",
    "        \n",
    "        with tf.name_scope(self.name + '_history'):\n",
    "            self.var = var\n",
    "            self.replicas = [] #this taks 2X memory\n",
    "            self.aplha = []\n",
    "            self.last_snapshot = tf.Variable(var.initialized_value(), name='snapshot') #this makes it 3X + hSize\n",
    "            self.next_idx = 0\n",
    "            self.op_cache = {}\n",
    "            self.o = None\n",
    "            \n",
    "            with tf.name_scope('replicas'):\n",
    "                for i in range(hSize):\n",
    "                    self.replicas.append(tf.Variable(np.zeros(var.get_shape()),\\\n",
    "                        dtype=var.dtype.base_dtype, name='replica'))\n",
    "                    \n",
    "            with tf.name_scope('alphas'):\n",
    "                for i in range(hSize):\n",
    "                    self.aplha.append(tf.Variable(np.zeros(1), dtype=var.dtype.base_dtype, name='alpha'))\n",
    "                    summaryMgr.add_iter_summary(tf.summary.histogram('alphas', self.aplha[-1]))\n",
    "                \n",
    "\n",
    "\n",
    "            for i in range(hSize):\n",
    "                self.push_history_op() #make sure all ops are created\n",
    "\n",
    "            HVar.all_hvars.append(self)\n",
    "            assert(self.next_idx == 0)\n",
    "\n",
    "        \n",
    "    def out(self):\n",
    "        if self.o is not None:\n",
    "            return self.o\n",
    "        \n",
    "        with tf.name_scope(self.name + '_out'):\n",
    "            #return an affine combination of the history vectors\n",
    "            #and a dictonary to add to feed_dict.\n",
    "            self.o = self.var\n",
    "            for r, a in zip(self.replicas, self.aplha):\n",
    "                self.o += r*a\n",
    "\n",
    "            return self.o\n",
    "        \n",
    "    #returns an op that updates history and snapshot (executed after optimization on alpha)\n",
    "    #This must be called when alpahs are non zeros!!!\n",
    "    def push_history_op(self):\n",
    "        if self.next_idx not in self.op_cache:\n",
    "            print 'HVar Cache Miss, creating the op for var ' + str(self.var.name) + ', idx = ' + str(self.next_idx)\n",
    "            sys.stdout.flush()\n",
    "            with tf.name_scope(self.name + '_update'):\n",
    "                \n",
    "                #first we update the original variable to the sesop result\n",
    "                update_var_op = tf.assign(self.var, self.out())\n",
    "                with tf.control_dependencies([update_var_op]):\n",
    "                    #now we update the history (self.var contain the sesop result):\n",
    "                    update_history_op = tf.assign(self.replicas[self.next_idx], self.var - self.last_snapshot)\n",
    "                    with tf.control_dependencies([update_history_op]):\n",
    "                        #now we update the last_snapshot to be the sesop result\n",
    "                        update_snapshot_op = tf.assign(self.last_snapshot, self.var)\n",
    "                        with tf.control_dependencies([update_snapshot_op]):\n",
    "                            #finally we reset all the alphas (infact we can take this out of the dependecy)\n",
    "                            #as it only affect self.out()\n",
    "                            reset_alpha_op = self.zero_alpha_op()            \n",
    "                            self.op_cache[self.next_idx] =\\\n",
    "                                tf.group(update_history_op, update_var_op, update_snapshot_op, reset_alpha_op)\n",
    "            \n",
    "        old_idx = self.next_idx\n",
    "        self.next_idx = (self.next_idx + 1)%len(self.replicas)\n",
    "\n",
    "        return self.op_cache[old_idx]\n",
    "    \n",
    "    def zero_alpha_op(self):\n",
    "        group_op = tf.no_op()\n",
    "        for a in self.aplha:\n",
    "            group_op = tf.group(group_op, tf.assign(a, np.zeros(1)))\n",
    "        return group_op\n",
    "        \n",
    "    #the alphas from sesop (the coefitients that choose the history vector)\n",
    "    @classmethod\n",
    "    def all_trainable_alphas(self):\n",
    "        alphas = []\n",
    "        for hvar in HVar.all_hvars:\n",
    "            alphas.extend(hvar.aplha)\n",
    "        return alphas\n",
    "    \n",
    "    #all the regular weights to be trained\n",
    "    @classmethod\n",
    "    def all_trainable_weights(self):\n",
    "        weights = []\n",
    "        for hvar in HVar.all_hvars:\n",
    "            weights.append(hvar.var)\n",
    "        return weights\n",
    "    \n",
    "    @classmethod\n",
    "    def all_history_update_ops(self):\n",
    "        group_op = tf.no_op()\n",
    "        for hvar in HVar.all_hvars:\n",
    "            group_op = tf.group(group_op, hvar.push_history_op())\n",
    "            \n",
    "        return group_op\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class SeboostOptimizer:\n",
    "    #batched_input, batched_labels are tensors that prodece batches\n",
    "    #is_training is a tensor that will be true while training and false while testing\n",
    "    #we run CG once in sesop_freq iterations \n",
    "    def __init__(self, loss, batched_input, batched_labels, sgd_steps):\n",
    "        \n",
    "        self.loss = loss\n",
    "        self.train_loss = []\n",
    "        self.loss_before_sesop = []\n",
    "        self.loss_after_sesop = []\n",
    "        self.sgd_steps = sgd_steps\n",
    "        self.iteration_ran = 0 \n",
    "        self.sesop_iteration_ran = 0\n",
    "        self.avg_gain_from_cg = 0.0\n",
    "        self.iter_summaries = summaryMgr.merge_iters()\n",
    "        \n",
    "        self.train_step = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss, name='minimizer',\\\n",
    "            var_list=HVar.all_trainable_weights())\n",
    "        \n",
    "        self.cg_var_list = HVar.all_trainable_alphas()\n",
    "        self.cg = tf.contrib.opt.ScipyOptimizerInterface(loss, var_list=self.cg_var_list,\\\n",
    "            method='CG', options={'maxiter':5})\n",
    "        \n",
    "        #all_trainable_weights\n",
    "        self.batched_input, self.batched_labels = batched_input, batched_labels\n",
    "        \n",
    "    #_feed_dict is the feed_dict needed to run regular sgd iteration\n",
    "    #sesop_feed_dict should contain feeds for the batch sesop will use!\n",
    "    #return a list of train_loss. The last elment in the list contain the loss after sesop.\n",
    "    def run_sesop_iteration(self, sess, _feed_dict, sesop_feed_dict):\n",
    "        #run sesop_freq SGD iterations:\n",
    "        if self.iteration_ran%self.sgd_steps != 0 or len(self.cg_var_list) == 0:\n",
    "            _, loss = sess.run([self.train_step, self.loss], feed_dict=_feed_dict)\n",
    "            self.iteration_ran += 1\n",
    "            \n",
    "            self.train_loss.append(loss)\n",
    "            self.writer.add_summary(sess.run(self.iter_summaries, feed_dict=_feed_dict), self.iteration_ran)\n",
    "            return loss\n",
    "\n",
    "        \n",
    "        self.loss_before_sesop.append(sess.run(self.loss, feed_dict=sesop_feed_dict))\n",
    "        #run 1 CG iteration\n",
    "        #print 'sess = ' + str(sess)\n",
    "        #print 'sesop_feed_dict = ' + str(sesop_feed_dict)\n",
    "        self.cg.minimize(sess, feed_dict=sesop_feed_dict)\n",
    "        self.iteration_ran += 1\n",
    "        self.sesop_iteration_ran += 1\n",
    "        \n",
    "        self.loss_after_sesop.append(sess.run(self.loss, feed_dict=sesop_feed_dict))\n",
    "        \n",
    "        self.avg_gain_from_cg += self.loss_before_sesop[-1] - self.loss_after_sesop[-1] \n",
    "        print 'Gain from CG: ' + str(self.avg_gain_from_cg/(self.sesop_iteration_ran))\n",
    "        sys.stdout.flush()\n",
    "        self.train_loss.append(self.loss_after_sesop[-1])\n",
    "        \n",
    "        #We want to capture the values of alpha before we zero them, so we need to call\n",
    "        #the summary before we zero them in self.history_update_ops\n",
    "        self.writer.add_summary(sess.run(self.iter_summaries, feed_dict=_feed_dict), self.iteration_ran)\n",
    "        \n",
    "        #Now when alphas are optimized, run the update history ops:\n",
    "        sess.run(HVar.all_history_update_ops())\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "        return self.loss_after_sesop[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fc_layer(input, n_in, n_out, log, hSize):\n",
    "    with tf.name_scope('FC'):\n",
    "        if log:\n",
    "            W = HVar(tf.Variable(tf.random_normal([n_in, n_out]), name='W'), hSize)\n",
    "            b = HVar(tf.Variable(tf.zeros([n_out]), name='b'), hSize)\n",
    "            a = tf.matmul(input, W.out()) + b.out()\n",
    "        else:\n",
    "            W = tf.Variable(tf.random_normal([n_in, n_out]), name='W')\n",
    "            b = tf.Variable(tf.zeros([n_out]), name='b')\n",
    "            a = tf.matmul(input, W) + b\n",
    "\n",
    "        out = tf.nn.tanh(a)\n",
    "        \n",
    "        if log:\n",
    "            summaryMgr.add_iter_summary(tf.summary.histogram('activations_before_tanh', a))\n",
    "            summaryMgr.add_iter_summary(tf.summary.histogram('activations_after_tanh', out))\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_model(x, y, dim, log=False, hSize=0):\n",
    "    layers = [fc_layer(x, dim, dim, log, hSize)]\n",
    "    for i in range(1):\n",
    "        layers.append(fc_layer(layers[-1], dim, dim, log, hSize))\n",
    "    layers.append(fc_layer(layers[-1], dim, 1, log, hSize))\n",
    "\n",
    "    model_out = layers[-1]\n",
    "\n",
    "\n",
    "    \n",
    "    #when log is true we build a model for training!\n",
    "    if log:\n",
    "        loss_per_sample = tf.squared_difference(model_out, y, name='loss_per_sample')\n",
    "        loss = tf.reduce_mean(loss_per_sample, name='loss')\n",
    "        summaryMgr.add_iter_summary(tf.summary.scalar('loss', loss))\n",
    "\n",
    "        return model_out, loss\n",
    "    #tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    return model_out #, loss, train_step\n",
    "\n",
    "#This class completly defines the Experiment:\n",
    "#1. Graph (the model)\n",
    "#2. optimizer\n",
    "#3. everything, so we can simply run it with a session\n",
    "#4. To achive this it will implement method: get_train_step_op\n",
    "#5. get_loss_op\n",
    "#6. get_set_train_op\n",
    "\n",
    "class Experiment:\n",
    "    is_training = tf.placeholder(tf.bool,name='is_training') #must be feed with dict_feed.\n",
    "    \n",
    "    def create_training_dataset(bs):\n",
    "        inputs = tf.cast(tf.constant(training_data, name='train_dataset_x'), tf.float32)\n",
    "        labels = tf.cast(tf.constant(training_labels, name='train_dataset_y'), tf.float32)\n",
    "        input, label = tf.train.slice_input_producer([inputs, labels], name='train_slicer')\n",
    "        batched_input, batched_labels = \\\n",
    "            tf.train.batch([input, label], batch_size=bs, name='train_batcher')\n",
    "        return batched_input, batched_labels\n",
    "\n",
    "\n",
    "    def create_testing_dataset(bs):\n",
    "        inputs = tf.cast(tf.constant(testing_data, name='test_dataset_x'), tf.float32)\n",
    "        labels = tf.cast(tf.constant(testing_labels, name='test_dataset_y'), tf.float32)\n",
    "\n",
    "        input, label = tf.train.slice_input_producer([inputs, labels], name='test_slicer')\n",
    "        batched_input, batched_labels = \\\n",
    "            tf.train.batch([input, label], batch_size=bs, name='test_batcher')\n",
    "        return batched_input, batched_labels\n",
    "            \n",
    "    \n",
    "    def loss(self):\n",
    "        return loss\n",
    "    \n",
    "    def update_step(self):\n",
    "        if self.optimizer.iteration_ran%self.optimizer.sgd_steps != 0 or len(self.optimizer.cg_var_list) == 0:\n",
    "            return self.train_step\n",
    "        \n",
    "            _, loss = sess.run([self.train_step, self.loss], feed_dict=_feed_dict)\n",
    "            self.iteration_ran += 1\n",
    "            \n",
    "            self.train_loss.append(loss)\n",
    "            self.writer.add_summary(sess.run(self.iter_summaries, feed_dict=_feed_dict), self.iteration_ran)\n",
    "            return loss\n",
    "        \n",
    "    def __init__(self, batch_size, training_data, testing_data, training_labels, testing_labels,\\\n",
    "                sesop_freq, hSize, writer):\n",
    "        dim = 10\n",
    "        \n",
    "        #create new summaryMgr:\n",
    "        summaryMgr.push_model(self)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.hSize = hSize\n",
    "        self.sesop_freq = sesop_freq\n",
    "        self.training_data = training_data\n",
    "        self.testing_data = testing_data\n",
    "        self.training_labels = training_labels\n",
    "        self.testing_labels = testing_labels\n",
    "        \n",
    "        with tf.name_scope('data'):\n",
    "            #It is very important to call create_training_dataset and create_testing_dataset \n",
    "            #create all queues (for train and test)\n",
    "            train_batched_input, train_batched_labels = create_training_dataset(batch_size)\n",
    "            test_batched_input, test_batched_labels = create_testing_dataset(batch_size)\n",
    "\n",
    "            self.batched_input, self.batched_labels = tf.cond(Experiment.is_training, lambda: [train_batched_input, train_batched_labels],\\\n",
    "                lambda: [test_batched_input, test_batched_labels])\n",
    "            \n",
    "        with tf.name_scope('model'):\n",
    "            self.model_out, self.loss = build_model(batched_input, batched_labels, dim, True, hSize)\n",
    "        \n",
    "\n",
    "        #hold acc loss\n",
    "        with tf.name_scope('loss_accamulator'):\n",
    "            acc_loss = tf.Variable(0, name='acc_loss', dtype=tf.float32)\n",
    "            self.train_loss_summary = tf.summary.scalar('train_loss', acc_loss)\n",
    "            self.test_loss_summary = tf.summary.scalar('test_loss', acc_loss)\n",
    "            \n",
    "        self.sgd_steps = int(1/sesop_freq)\n",
    "        self.optimizer = SeboostOptimizer(self.loss, batched_input, batched_labels, self.sgd_steps)\n",
    "        self.optimizer.writer = writer\n",
    "        \n",
    "        summaryMgr.pop_model(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_random_data(dim, n=5000):\n",
    "    cov = np.random.rand(dim, dim)\n",
    "    cov = np.dot(cov, cov.transpose())\n",
    "\n",
    "    training_data = np.random.multivariate_normal(np.zeros(dim), cov, n)\n",
    "    testing_data = np.random.multivariate_normal(np.zeros(dim), cov, n)\n",
    "    \n",
    "    with tf.name_scope('generating_data'):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, dim], name='x')\n",
    "        model_out = build_model(x, None, dim, False)\n",
    "\n",
    "        #with tf.Session('grpc://' + tf_server, config=config) as sess:\n",
    "        config = tf.ConfigProto()\n",
    "        #config.gpu_options.allow_growth = True\n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            training_labels = sess.run(model_out, feed_dict={x: training_data})\n",
    "            testing_labels = sess.run(model_out, feed_dict={x: testing_data})\n",
    "\n",
    "        return training_data, testing_data, training_labels, testing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bs is batch size\n",
    "#sesop_freq is in (0,1) and is the fraction of sesop iterations.\n",
    "#i.e., if sesop_freq = 0.1 then we do 1 sesop iteration for each one sgd iteration\n",
    "#epochs is the number of epochs\n",
    "\n",
    "#bs is batch size\n",
    "#sesop_freq is in (0,1) and is the fraction of sesop iterations.\n",
    "#i.e., if sesop_freq = 0.1 then we do 1 sesop iteration for each one sgd iteration\n",
    "#epochs is the number of epochs\n",
    "def run_experiment(bs, sesop_freq, hSize, epochs, file_writer_suffix):\n",
    "    dim = 10\n",
    "\n",
    "\n",
    "    training_data, testing_data, training_labels, testing_labels = generate_random_data(dim, 5000)\n",
    "    print training_data.shape\n",
    "    print testing_data.shape\n",
    "\n",
    "    #self, batch_size, training_data, testing_data, training_labels, testing_labels,\\\n",
    "    #            sesop_freq, hSize, writer\n",
    "    \n",
    "    \n",
    "    writer = tf.summary.FileWriter('/tmp/generated_data/' + file_writer_suffix)\n",
    "\n",
    "    experiments = [Experiment(batch_size, training_data, testing_data, training_labels, testing_labels,\\\n",
    "              sesop_freq, hSize, writer)]\n",
    "\n",
    "    \n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #with tf.Session('grpc://' + tf_server, config=config) as sess:\n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        iters_per_epoch = 5000/bs\n",
    "        sgd_steps = int(1/sesop_freq)\n",
    "\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "\n",
    "        #we must start queue_runners\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            #run 20 steps (full batch optimization to start with)\n",
    "            print 'epoch #' + str(epoch)\n",
    "            print 'Computing train error'\n",
    "            total_loss = 0\n",
    "            for i in range(iters_per_epoch):\n",
    "                iter_loss = sess.run([e.loss() for e in experiments], feed_dict={Experiment.is_training: True})\n",
    "                total_loss += iter_loss\n",
    "                \n",
    "            print 'Train Error: ' + str(total_loss/(5000/bs))\n",
    "            #put the accamulated loss into acc_loss node\n",
    "            #writer.add_summary(sess.run(train_loss_summary, feed_dict={acc_loss: total_loss/(5000/bs)}), epoch)\n",
    "            \n",
    "            print 'Computing test error'\n",
    "            total_loss = 0\n",
    "            for i in range(iters_per_epoch):\n",
    "                iter_loss = sess.run([e.loss() for e in experiments], feed_dict={Experiment.is_training: False})\n",
    "                total_loss += iter_loss\n",
    "            #put the accamulated loss into acc_loss node\n",
    "            #writer.add_summary(sess.run(test_loss_summary, feed_dict={acc_loss: total_loss/(5000/bs)}), epoch)\n",
    "            print 'Test Error: ' + str(total_loss/(5000/bs))\n",
    "            \n",
    "            \n",
    "            print 'Training'\n",
    "            total_loss = 0\n",
    "            for i in range(iters_per_epoch):\n",
    "                #take a batch for sesop:\n",
    "                batched_input_actual, batched_labels_actual = \\\n",
    "                    sess.run([[e.batched_input, e.batched_labels] for e in experiments],\\\n",
    "                        feed_dict={Experiment.is_training: True})\n",
    "\n",
    "\n",
    "                #this runs 1 iteration and keeps track of when should it do sesop.\n",
    "                iter_loss = optimizer.run_sesop_iteration(sess=sess, _feed_dict={Experiment.is_training: True} ,\\\n",
    "                    sesop_feed_dict=\\\n",
    "                    {is_training: True, batched_input: batched_input_actual, batched_labels: batched_labels_actual})\n",
    "\n",
    "\n",
    "            #writer.flush()\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10)\n",
      "(5000, 10)\n",
      "HVar Cache Miss, creating the op for var FC_1/W:0, idx = 0\n",
      "HVar Cache Miss, creating the op for var FC_1/W:0, idx = 1\n",
      "HVar Cache Miss, creating the op for var FC_1/b:0, idx = 0\n",
      "HVar Cache Miss, creating the op for var FC_1/b:0, idx = 1\n",
      "HVar Cache Miss, creating the op for var FC_2/W:0, idx = 0\n",
      "HVar Cache Miss, creating the op for var FC_2/W:0, idx = 1\n",
      "HVar Cache Miss, creating the op for var FC_2/b:0, idx = 0\n",
      "HVar Cache Miss, creating the op for var FC_2/b:0, idx = 1\n",
      "HVar Cache Miss, creating the op for var FC_3/W:0, idx = 0\n",
      "HVar Cache Miss, creating the op for var FC_3/W:0, idx = 1\n",
      "HVar Cache Miss, creating the op for var FC_3/b:0, idx = 0\n",
      "HVar Cache Miss, creating the op for var FC_3/b:0, idx = 1\n",
      "epoch #0\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: 0.0\n",
      "epoch #1\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "epoch #2\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: 0.0\n",
      "epoch #3\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "epoch #4\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: 0.00225003560384\n",
      "epoch #5\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "epoch #6\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: 0.00453612953424\n",
      "epoch #7\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "epoch #8\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n",
      "Gain from CG: 0.014226397872\n",
      "epoch #9\n",
      "Computing train error\n",
      "Computing test error\n",
      "Training\n"
     ]
    }
   ],
   "source": [
    "summaryMgr = SummaryManager()\n",
    "#def run_experiment(bs, sesop_freq, hSize, epochs, file_writer_suffix)\n",
    "run_experiment(bs=100, sesop_freq=0.01, hSize=2, epochs=10, file_writer_suffix='3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}